import random
import gym
import numpy as np
from collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import itertools
import functools
from gym.envs.registration import registry, register, make, spec
from ddqlearner_three import flatten_state_withtime as flatten_state
from ddqlearner_three import get_alt_action_size as get_action_size
from ddqlearner_three import alt_action_lookup
# Deep Q-learning Agent
from keras.models import load_model
class DDQNAgent:
    def __init__(self, env,n_episodes,fname, max_env_steps=None):
        self.env = env
        self.n_episodes = n_episodes
        self.state_size = len(flatten_state(env.observation_space.sample())[0])
        self.action_size = get_action_size(self.env)
        self.action_lookup = functools.partial(alt_action_lookup,env)#list(itertools.product(*(range(space.n) for space in env.action_space.spaces)))
        self.epsilon = 0.01
        self.model = load_model(fname)
        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps
    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])  # returns action
    def run(self, render=True):
        for e in range(self.n_episodes):
            print('#######New episode#############', self.epsilon)
            done=False
            observation = self.env.reset()
            prev_state = flatten_state(observation)
            reward_sum = 0
            i=0
            while not done and i<self.env._max_episode_steps:
                if render: self.env.render()
                action = agent.act(prev_state)
                # record various intermediates (needed later for backprop)
                # step the environment and get new measurements
                observation, reward, done, info = self.env.step(self.action_lookup(observation, action))
                flat_state = flatten_state(observation)
                # Remember the previous state, action, reward, and done
                # make next_state the new current state for the next frame.
                prev_state = flat_state
                reward_sum += reward
                i+=1
            print("episode: {}/{}, score: {}".format(e, self.n_episodes, reward_sum))
            #agent.replay(32)
        return e

if __name__ == "__main__":
    # initialize gym environment and the agent
    import sys
    nepisodes = int(sys.argv[1])
    fname = sys.argv[2]
    register(
    id='MultiSensor-v0',
    entry_point='multi_sensor_env:MultiSensorEnv',
    kwargs = {'num_sensors':4}
    )
    env = gym.make('MultiSensor-v0')
    agent = DDQNAgent(env,n_episodes = nepisodes,fname=fname, max_env_steps=200)
    agent.run()