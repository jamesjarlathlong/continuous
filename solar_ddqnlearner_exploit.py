import random
import gym
import numpy as np
from collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import itertools
import functools
from gym.envs.registration import registry, register, make, spec
from ddqlearner import flatten_state_withtime, get_action_size, get_alt_action_size, alt_action_lookup, action_lookup, _huber_loss
# Deep Q-learning Agent
from keras.models import load_model
import simple_solar_env
import solar_sensor_env
import functools
class DDQNAgent:
    def __init__(self, env,n_episodes,fname, max_env_steps=None):
        self.env = env
        self.n_episodes = n_episodes
        self.state_size = len(flatten_state_withtime(env.observation_space.sample())[0])
        self.action_size = get_alt_action_size(self.env)
        self.action_lookup = functools.partial(alt_action_lookup, env)
        #list(itertools.product(*(range(space.n) for space in env.action_space.spaces)))
        self.epsilon = 0.01
        self.model = load_model(fname,custom_objects = {'_huber_loss':_huber_loss})
        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps
    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])  # returns action
    def run(self, render=True):
        for e in range(self.n_episodes):
            print('#######New episode#############', self.epsilon)
            done=False
            observation = self.env.reset()
            prev_state = flatten_state_withtime(observation)
            reward_sum = 0
            i=0
            while not done and i<self.env._max_episode_steps:
                if render: self.env.render()
                action = self.act(prev_state)
                # record various intermediates (needed later for backprop)
                # step the environment and get new measurements
                observation, reward, done, info = self.env.step(self.action_lookup(observation, action))
                flat_state = flatten_state_withtime(observation)
                # Remember the previous state, action, reward, and done
                # make next_state the new current state for the next frame.
                prev_state = flat_state
                reward_sum += reward
                i+=1
            print("episode: {}/{}, score: {}".format(e, self.n_episodes, reward_sum))
            #agent.replay(32)
        return e

if __name__ == "__main__":
    # initialize gym environment and the agent
    import sys
    nepisodes = int(sys.argv[1])
    fname = sys.argv[2]
    recordname = sys.argv[3]
    #solarrecord = simple_solar_env.emulate_solar_ts(365)
    solarfname = 'training_12'
    #solarfname='testing'
    solarrecord = solar_sensor_env.get_generated_power(solarfname)
    register(
    id='SolarTimeSensor-v0',
    entry_point='solar_sensor_env:SolarTimeSensorEnv',
    kwargs = {'max_batt':10,'num_sensors':16,'deltat':3,'solarpowerrecord':solarrecord, 'recordname':recordname}
    )
    env = gym.make('SolarTimeSensor-v0')
    agent = DDQNAgent(env,n_episodes = nepisodes,fname=fname, max_env_steps=365*8)
    agent.run()
